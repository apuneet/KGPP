{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stqa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/apuneet/KGPP/blob/master/stqa.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yFYPeVI45Mo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import callbacks\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as ke\n",
        "import os\n",
        "import sys\n",
        "import h5py\n",
        "from shutil import copyfile\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "lfn = None\n",
        "cos_sim, tf_sess, a, b = None, None, None, None\n",
        "is_theano = False\n",
        "job_name = 'job-1'\n",
        "load_legend = ['load from saved_data', 'save to saved_data', 'pre-process, but don\\'t save or load']\n",
        "p = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRo2O3UH5eHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_params(conf_pf):\n",
        "    global p\n",
        "    sint = ignore_exception(ValueError)(int)\n",
        "    sfloat = ignore_exception(ValueError)(float)\n",
        "    with open(conf_pf) as f:\n",
        "        content = f.readlines()\n",
        "    for next_line in content:\n",
        "        if next_line.startswith('#') or next_line == '\\n' or len(next_line) == 0:\n",
        "            continue\n",
        "        toks = next_line.split('=')\n",
        "        val = toks[1].replace('\\n', '')\n",
        "        print toks[0] + '=' + str(val)\n",
        "        if sint(val) is not None:\n",
        "            p[toks[0]] = sint(val)\n",
        "            continue\n",
        "        if sfloat(val) is not None:\n",
        "            p[toks[0]] = sfloat(val)\n",
        "            continue\n",
        "        p[toks[0]] = val\n",
        "    lod_home = 'data/Work-Homes/LOD_HOME/'\n",
        "    p['dp_home'] = os.path.join(lod_home, p['dp_name'])\n",
        "    my_args = MyArgs(p)\n",
        "    jf, jn = set_job_folder(os.path.join(p['dp_home'], 'model/kgt1'))\n",
        "    p['job_folder'] = jf\n",
        "    p['job_number'] = jn\n",
        "    p['conf_file'] = conf_pf\n",
        "    my_args.set_details(jf, is_theano)\n",
        "    copyfile(conf_pf, os.path.join(p['job_folder'], os.path.basename(conf_pf)))\n",
        "    print '========================================================='\n",
        "    return my_args\n",
        "\n",
        "\n",
        "def set_job_folder(base_folder='/data/Work-Homes/LOD_HOME/fb15k2/model/kgt1/'):\n",
        "    max_ct = 0\n",
        "    for next_file in os.listdir(base_folder):\n",
        "        if next_file.startswith('job-'):\n",
        "            next_ct = int(next_file.replace('job-', ''))\n",
        "            max_ct = max(max_ct, next_ct)\n",
        "    job_number = int(max_ct+1)\n",
        "    tb_log_folder = os.path.join(base_folder, 'job-' + str(job_number))\n",
        "    print 'tb_log_folder=' + tb_log_folder\n",
        "    os.mkdir(tb_log_folder)\n",
        "    return tb_log_folder, job_number\n",
        "\n",
        "\n",
        "class MyArgs:\n",
        "    def __init__(self, ip):\n",
        "        self.p = ip\n",
        "        self.edge_list_pf = os.path.join(p['dp_home'], 'wip-data-3/converted-rdf-' + p['dp_name'] + '.rdf')\n",
        "        self.job_folder = '/data/Work-Homes/LOD_HOME/fb15k2/model/kgt1/try/'\n",
        "        self.word_embedding = os.path.join(ip['dp_home'], ip['embedding_suffix'])\n",
        "        self.input_path = os.path.join(ip['dp_home'], ip['input_suffix'])\n",
        "        self.is_theano = False\n",
        "        #os.path.isdir(self.job_folder) or os.mkdir(self.job_folder)\n",
        "        #self.input_path = '/data/Work-Homes/LOD_HOME/fb15k2/wip-data/query_prep/ready_question_sets/set-2'\n",
        "\n",
        "    def set_details(self, jf, is_theano_used):\n",
        "        self.job_folder = jf\n",
        "        self.is_theano = is_theano_used\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oZ1V-ex05-Fg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(conf_file):\n",
        "    my_args = parse_params(conf_file)\n",
        "    em_dim, word_ind, em_mat, isl, x_all, np_y_all, y_all, src_ent_all, osl, jos, qs, \\\n",
        "    rel_tok_seqs, rel_ids = load_saved_data(my_args)\n",
        "    print_basics(em_dim, len(word_ind), len(em_mat), isl, osl)\n",
        "    if p['load_command'] == 1:\n",
        "        return\n",
        "    x_train, y_train, x_valid, y_valid = x_all[0], np_y_all[0], x_all[1], np_y_all[1]\n",
        "    print 'x_train.shape=' + str(x_train.shape)\n",
        "    #print 'y_train.shape=' + str(y_train.shape)\n",
        "    print 'isl=' + str(isl)\n",
        "    store_settings(my_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLL9lB8k86hn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "outputId": "8a596153-dc5d-49b5-c95e-9adb91d1a0ff"
      },
      "cell_type": "code",
      "source": [
        "main('kgt.conf')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dp_name=subgraph_webq\n",
            "input_suffix=input_data/split-7\n",
            "embedding_suffix=model/ere-glove/m-10.embd\n",
            "use_ere=1\n",
            "load_command=0\n",
            "model_number=13\n",
            "epoch_count=1\n",
            "learn_embed=0\n",
            "dropout=0.0\n",
            "rec_dropout=0.0\n",
            "l1=10\n",
            "l2=6\n",
            "l3=256\n",
            "l4=256\n",
            "pos_loss_wt=0.7\n",
            "top_k=1\n",
            "beam_size=1\n",
            "none_val=0\n",
            "tb_log_folder=data/Work-Homes/LOD_HOME/subgraph_webq/model/kgt1/job-4\n",
            "=========================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f9986b4d53b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kgt.conf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#!mv word_index.pickle data/Work-Homes/LOD_HOME/subgraph_webq/model/kgt1/saved_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-568c9925cdde>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(conf_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmy_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mem_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_y_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_ent_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mosl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mrel_tok_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_saved_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint_basics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mosl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'load_command'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-4f365471111d>\u001b[0m in \u001b[0;36mload_saved_data\u001b[0;34m(this_args)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mop_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_index.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0membed_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embedding_matrix.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx_all_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_all_padded.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnp_y_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'np_y_all.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-effd9c6589f3>\u001b[0m in \u001b[0;36mload_pickle_file\u001b[0;34m(path_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/content/data/Work-Homes/LOD_HOME/subgraph_webq/model/kgt1/saved_data/embedding_matrix.pickle'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kqABt7Hy7zwf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model13(p, em_mat, word_ind, em_dim, isl, osl):\n",
        "    \"\"\"\n",
        "    Bi-LSTM, BatchNorm, MaxPool, 1-Layer Enc + BN, 1-Layer-Dec + BN + TDD-tanh\n",
        "    \"\"\"\n",
        "    do = p['dropout']\n",
        "    rdo = p['rec_dropout']\n",
        "    fine_tune = True if p['learn_embed'] == 1 else False\n",
        "    si = Input(shape=(isl,), dtype='int32')\n",
        "    embedding_layer = Embedding(input_dim=len(word_ind) + 1, output_dim=em_dim,\n",
        "                                input_length=isl, weights=[em_mat], trainable=fine_tune)(si)\n",
        "\n",
        "    encoded = Bidirectional(LSTM(p['l1'], dropout=do, recurrent_dropout=rdo, return_sequences=True))(embedding_layer)\n",
        "    bn_enc = normalization.BatchNormalization()(encoded)\n",
        "\n",
        "    att = Dense(100, activation='softmax')(bn_enc)\n",
        "    #pool_rnn = Lambda(lambda x: ke.max(x, axis=1))(bn_enc)\n",
        "    #decode_inp = RepeatVector(osl)(att)\n",
        "    #print 'em_dim=' + str(em_dim)\n",
        "\n",
        "    decoded = Bidirectional(LSTM(p['l2'], dropout=do, recurrent_dropout=rdo, return_sequences=True))(att)\n",
        "    td = TimeDistributed(Dense(em_dim, activation='tanh'))(decoded)\n",
        "\n",
        "    s2s_model = Model(inputs=[si], outputs=[td])\n",
        "    print 'Starting to compile the model ...'\n",
        "    s2s_model.compile(optimizer='adam', loss=myloss)\n",
        "    return s2s_model, True\n",
        "\n",
        "\n",
        "def myloss(y_true, y_pred):\n",
        "    v1 = y_pred\n",
        "    v2 = y_true\n",
        "    numerator = ke.sum(v1 * v2)\n",
        "    denominator = ke.sqrt(ke.sum(v1 ** 2) * ke.sum(v2 ** 2))\n",
        "    loss = abs(1 - numerator/denominator)\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qvl5toC06CBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_model(args, em_mat, word_ind, em_dim, isl, osl, x_train, y_train, x_valid, y_valid, x_test,\n",
        "              rel_tok_seqs, rel_ids, y_all, src_ent_all, jos, qs):\n",
        "    mods = get_model_dict()\n",
        "    my_predictions_trn, my_predictions_tst = None, None\n",
        "    \n",
        "    my_mod, to_fp = get_model13(p, em_mat, word_ind, em_dim, isl, osl)\n",
        "    my_predictions_trn, my_predictions_tst, emat, ygt = fit_and_predict(args, my_mod, x_train, y_train, x_valid, y_valid, x_test)    \n",
        "    return my_predictions_trn, my_predictions_tst, emat, ygt\n",
        "\n",
        "\n",
        "def fit_and_predict(my_args, my_model, x_train, y_train, x_valid, y_valid, x_test):\n",
        "    plot_model(my_model, to_file=os.path.join(my_args.job_folder, 'model-' + str(p['job_number']) + '.png'))\n",
        "    tb = callbacks.TensorBoard(log_dir=my_args.job_folder, histogram_freq=0, batch_size=32, write_graph=False,\n",
        "                               write_grads=True, write_images=False, embeddings_freq=0,\n",
        "                               embeddings_layer_names=None, embeddings_metadata=None)\n",
        "    fp = my_args.job_folder + '/weights.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "    mc = callbacks.ModelCheckpoint(filepath=fp, save_best_only=True, mode='auto')\n",
        "    print 'Starting to fit the model ...' + str(my_args.p['model_number'])\n",
        "    if 'weight_file' in p.keys():\n",
        "        fn = os.path.join(p['job_folder'], p['weight_file'])\n",
        "        my_model.load_weights(fn)\n",
        "    elif len(x_valid) < 5:\n",
        "        if my_args.p['model_number'] != 12:\n",
        "            my_model.fit(x=x_train, y=y_train, epochs=p['epoch_count'], verbose=1, callbacks=[tb])\n",
        "        else:\n",
        "            my_model.fit(x=x_train, y={'time_distributed_1': y_train[0], 'time_distributed_2': y_train[1]},\n",
        "                         epochs=p['epoch_count'], verbose=1, callbacks=[tb])\n",
        "    else:\n",
        "        print 'Length of x_valid arrary = ' + str(len(x_valid))\n",
        "        print 'y_train.shape=' + str(y_train.shape)\n",
        "        print 'y_valid.shape=' + str(y_valid.shape)\n",
        "        y_train1 = pad_sequences(y_train, maxlen=80, padding='post')\n",
        "        y_valid1 = pad_sequences(y_valid, maxlen=80, padding='post')\n",
        "        print 'y_train.shape=' + str(y_train1.shape)\n",
        "        print 'y_valid.shape=' + str(y_valid1.shape)\n",
        "        my_model.fit(x=x_train, y=y_train1, validation_data=(x_valid, y_valid1), epochs=p['epoch_count'], verbose=1,\n",
        "                     callbacks=[tb, mc])\n",
        "\n",
        "    print 'Starting to predict using the model ... with Tst Data First'\n",
        "    my_predictions_tst = my_model.predict(x=x_test, verbose=1)\n",
        "    print '\\nmy_predictions_tst.shape=' + str(my_predictions_tst.shape)\n",
        "    print 'my_predictions_tst[0].shape=' + str(my_predictions_tst[0].shape)\n",
        "    print 'my_predictions_tst[1].shape=' + str(my_predictions_tst[1].shape)\n",
        "\n",
        "    if my_predictions_tst.shape[1] > 2:\n",
        "        y_temp = list()\n",
        "        for i in range(len(my_predictions_tst)):\n",
        "            mod_pred = my_predictions_tst[i][:2]\n",
        "            y_temp.append(mod_pred)\n",
        "        np_t_temp = np.asarray(y_temp, dtype=float)\n",
        "        print 'new - my_predictions_tst.shape=' + str(np_t_temp.shape)\n",
        "        my_predictions_tst = np_t_temp\n",
        "\n",
        "    #my_predictions_trn = my_model.predict(x=x_train, verbose=1)\n",
        "    my_predictions_trn = my_predictions_tst # temporarily disabling this\n",
        "    if p['model_number'] == 12:\n",
        "        tst_pres = my_predictions_tst[0]\n",
        "        trn_pres = my_predictions_trn[0]\n",
        "    else:\n",
        "        tst_pres = my_predictions_tst\n",
        "        trn_pres = my_predictions_trn\n",
        "    return trn_pres, tst_pres, my_model.layers[1].get_weights()[0], None\n",
        "\n",
        "\n",
        "def save_predictions(my_args, np_y_all_pred, split_name):\n",
        "    print 'Entering save_predictions() - for ' + split_name\n",
        "    print 'len(np_y_all_pred)=' + str(len(np_y_all_pred))\n",
        "    print 'np_y_all_pred.shape=' + str(np_y_all_pred.shape)\n",
        "    op_file = bas_utils.open_file(os.path.join(my_args.job_folder, split_name + '_pred_embeddings.txt'))\n",
        "    for i in range(len(np_y_all_pred)):\n",
        "        next_pred_list = np_y_all_pred[i]\n",
        "        for pred_e in next_pred_list:\n",
        "            op_file.write(bas_utils.to_string(pred_e, ',') + ';')\n",
        "        op_file.write('\\n')\n",
        "    op_file.close()\n",
        "    print 'Exiting save_predictions() - for ' + split_name\n",
        "\n",
        "\n",
        "def myloss(y_true, y_pred):\n",
        "    v1 = y_pred\n",
        "    v2 = y_true\n",
        "    numerator = ke.sum(v1 * v2)\n",
        "    denominator = ke.sqrt(ke.sum(v1 ** 2) * ke.sum(v2 ** 2))\n",
        "    loss = abs(1 - numerator/denominator)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def print_basics(em_dim, len_word_ind, len_em_mat, isl, osl):\n",
        "    print '--------------------------------------------------------------------------'\n",
        "    print 'em_dim=' + str(em_dim)\n",
        "    print 'len_word_ind=' + str(len_word_ind)\n",
        "    print 'len_em_mat=' + str(len_em_mat)\n",
        "    print 'Input Sequence Length=' + str(isl)\n",
        "    print 'Output Sequence Length=' + str(osl)\n",
        "    print '--------------------------------------------------------------------------'\n",
        "    print '                              Loaded data '\n",
        "    print '--------------------------------------------------------------------------'\n",
        "\n",
        "\n",
        "def store_settings(args):\n",
        "    op_file = open(os.path.join(args.job_folder, 'params.txt'), 'w')\n",
        "    op_file.write('is_theano='+str(args.is_theano)+'\\n')\n",
        "    for k in p.keys():\n",
        "        op_file.write(k+'=' + str(p[k]) + '\\n')\n",
        "    op_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54QfkSkQ8E-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_saved_data(this_args):\n",
        "    op_path = os.path.abspath(os.path.join(this_args.job_folder, os.pardir, 'saved_data'))\n",
        "    word_index = load_pickle_file(os.path.join(op_path, 'word_index.pickle'))\n",
        "    embed_mat = load_pickle_file(os.path.join(op_path, 'embedding_matrix.pickle'))\n",
        "    x_all_padded = load_pickle_file(os.path.join(op_path, 'x_all_padded.pickle'))\n",
        "    np_y_all = load_pickle_file(os.path.join(op_path, 'np_y_all.pickle'))\n",
        "    y_all = load_pickle_file(os.path.join(op_path, 'y_all.pickle'))\n",
        "    src_ent_all = load_pickle_file(os.path.join(op_path, 'src_ent_all.pickle'))\n",
        "    rel_tok_seqs = load_pickle_file(os.path.join(op_path, 'rel_tok_seqs.pickle'))\n",
        "    rel_ids = load_pickle_file(os.path.join(op_path, 'rel_ids.pickle'))\n",
        "    embd_dim, max_sen_len, max_osl = 0, 0, 0\n",
        "    with open(os.path.join(op_path, 'vars.txt')) as f:\n",
        "        content = f.readlines()\n",
        "    for next_line in content:\n",
        "        if next_line.startswith('embd_dim='):\n",
        "            embd_dim = int(next_line.split('=')[1].replace('\\n', ''))\n",
        "        if next_line.startswith('max_sen_len='):\n",
        "            max_sen_len = int(next_line.split('=')[1].replace('\\n', ''))\n",
        "        if next_line.startswith('max_osl='):\n",
        "            max_osl = int(next_line.split('=')[1].replace('\\n', ''))\n",
        "    jos = list()\n",
        "    for fn in fns:\n",
        "        json_obj = bas_utils.load_json_file(os.path.join(op_path, fn))\n",
        "        jos.append(json_obj)\n",
        "    qs = load_pickle_file(os.path.join(op_path, 'qid_sqs.pickle'))\n",
        "    print 'Returning from Saved Data ...'\n",
        "    return embd_dim, word_index, embed_mat, max_sen_len, x_all_padded, np_y_all, y_all, src_ent_all, max_osl, jos, \\\n",
        "           qs, rel_tok_seqs, rel_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvIdq78k9dMp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ignore_exception(IgnoreException=Exception, DefaultVal=None):\n",
        "    \"\"\" Decorator for ignoring exception from a function\n",
        "    e.g.   @ignore_exception(DivideByZero)\n",
        "    e.g.2. ignore_exception(DivideByZero)(Divide)(2/0)\n",
        "    \"\"\"\n",
        "    def dec(function):\n",
        "        def _dec(*args, **kwargs):\n",
        "            try:\n",
        "                return function(*args, **kwargs)\n",
        "            except IgnoreException:\n",
        "                return DefaultVal\n",
        "        return _dec\n",
        "    return dec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijyVPiRN9tGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_pickle_file(path_file):\n",
        "    return pickle.load(open(path_file, \"rb\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8Wa2L-a95Xj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_json_file(path_file):\n",
        "    print 'Starting to load JSON File - ' + path_file\n",
        "    if not os.path.isfile(path_file):\n",
        "        raise ValueError('No Such file - ' + path_file)\n",
        "    ct, str_json,  = 0, ''\n",
        "    with open(path_file) as f:\n",
        "        content = f.readlines()\n",
        "    for next_line in content:\n",
        "        ct += 1\n",
        "        str_json += next_line\n",
        "        msg = 'load_json_file - ' + '     -->'\n",
        "        print_status(msg, ct, 10)\n",
        "    my_data_obj = json.loads(str_json)\n",
        "    return my_data_obj"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}